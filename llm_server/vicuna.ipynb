{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yui/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CUDA extension not installed.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n",
      "The model 'LlamaGPTQForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from transformers import AutoTokenizer, logging, pipeline\n",
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "model_id = \"TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g\"\n",
    "model_basename=\"vicuna-7B-1.1-GPTQ-4bit-128g.no-act-order\"\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_id,device=\"cuda:0\",use_safetensors=False, quantize_config=None, model_basename=model_basename)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "\n",
    "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=\"cuda:0\" , max_new_tokens = 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Tall me about AI and use a word to answer me\n",
      "\n",
      "### Assistant: \n",
      "\n",
      "1. Artificial Intelligence is a field of computer science that deals with the development of intelligent machines that work and react like humans.\n",
      "2. AI is used in various industries such as healthcare, finance, and education to automate tasks and make decisions.\n",
      "3. AI can be used to create chatbots, virtual assistants, and self-driving cars.\n",
      "4. AI can also be used to create personalized experiences for customers.\n",
      "5. The use of AI raises ethical questions such as the potential for job displacement and the need for transparency in decision-making.\n",
      "6. The future of AI is expected to bring more\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Tall me about AI and use a word to answer me\n",
    "\"\"\"\n",
    "\n",
    "prompt_template=f'''### Human: {prompt}\n",
    "### Assistant: \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "print(pipeline(prompt_template)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
